---
title: "Coursera Data Science Capstone Milestone Report"
author: "popotam"
date: "November 16, 2014"
output: html_document
---

Intro
-----

Around the world, people are spending an increasing amount of time on their
mobile devices for email, social networking, banking and a whole range
of other activities. But typing on mobile devices can be a serious pain.
SwiftKey, a corporate partner in this capstone,
builds a smart keyboard that makes it easier for people to type on their
mobile devices.

This report contains an exploratory analysis of the Swiftkey dataset
that will help us to better understand the data and later on create
a prediction model based on this dataset.


Data Loading and Calculation of Basic Summary
---------------------------------------------

### Load the Data

First let's load some libraries needed in the whole process and set random seed for reproducibility.

```{r libraries, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
# load libraries
library(knitr)  # for kable function

# set seed
set.seed(1234)
```

Next, let's download and unzip the Swiftkey dataset (files won't be downloaded if they are already available on the disk).

```{r download, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
# download and unzip data set once
if (!file.exists("Coursera-SwiftKey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
                  destfile = "Coursera-SwiftKey.zip", method = "curl")
}
if (!file.exists("final")) {
    unzip("Coursera-SwiftKey.zip")
}
```

The next block of code is longer, because the dataset is huge and uses a huge amount of memory.
In the next step we will do 3 things at the same time:

* load the dataset

* calculate the basic statistics

* prepare the 10000 lines sample from every data type

Loading data and extracting sample must be done in one block of code,
because we do not want to cache the entire dataset (which is over 583MB of text).
This way only the sample will be cached and the whole process of loading
the entire dataset will be done only once.

```{r preprocessing, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
# load data
blogs <- readLines(file("final/en_US/en_US.blogs.txt", encoding = "UTF-8"), n = 10000)
news <- readLines(file("final/en_US/en_US.news.txt", encoding = "UTF-8"), n = 10000)
twitter <- readLines(file("final/en_US/en_US.twitter.txt", encoding = "UTF-8"), n = 10000)
blogs <- iconv(blogs, from = "latin1", to = "UTF-8", sub="")
news <- iconv(news, from = "latin1", to = "UTF-8", sub="")
twitter <- iconv(twitter, from = "latin1", to = "UTF-8", sub="")

# calculate basic statistics
summary <- data.frame(
    row.names = c("blogs", "news", "twitter"),
    lines = c(
        length(blogs),
        length(news),
        length(twitter)
    ),
    words = c(
        sum(sapply(gregexpr("\\S+", blogs), length) + 1),
        sum(sapply(gregexpr("\\S+", news), length) + 1),
        sum(sapply(gregexpr("\\S+", twitter), length) + 1)
    ),
    characters = c(
        sum(sapply(blogs, nchar)),
        sum(sapply(news, nchar)),
        sum(sapply(twitter, nchar))
    ),
    whitespace = c(
        sum(sapply(gregexpr("\\s+", blogs), length) + 1),
        sum(sapply(gregexpr("\\s+", news), length) + 1),
        sum(sapply(gregexpr("\\s+", twitter), length) + 1)
    )
)
summary$words.per.line <- summary$words / summary$lines
summary$characters.per.line <- summary$characters / summary$lines
summary$characters.per.word <- (summary$characters - summary$whitespace) / summary$words

# extract samples from data
blogs = sample(blogs, 1000)
news = sample(news, 1000)
twitter = sample(twitter, 1000)
```

We have extracted a sample of 10000 lines from every data type: blogs, news and tweets.
This will allow us to analyze the dataset in feasible time.

### Summary

Having done this, let's see the basic summary of the dataset.

```{r basic_summary, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
# show basic statistics
kable(summary, format = "html", table.attr = "border=\"1\"")

```

As we can see, the average word length is very similar in all three data types (between 4 and 5 characters) and the only difference is in the line length. Tweeter data contains the shortest lines - this is due to their 140 character limit per tweet) and blogs contain the longest lines.

```{r chars_per_line_plot, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
par(mfrow = c(1, 2))
barplot(
    summary$characters.per.word,
    names.arg = row.names(summary),
    main = "Average Word Length",
    ylim = c(0, 6)
)
barplot(
    summary$characters.per.line,
    names.arg = row.names(summary),
    main = "Average Line Length",
    ylim = c(0, 300)
)
```

Data processing
---------------

Our sample of the raw data needs to be further processed in order to analyze the frequencies of particular words and n-grams (pairs or tripplets of words).

First, we load the libraries that are required to process the data:
```{r tm_libraries, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
library(tm)
library(slam)
library(RWeka)
library(wordcloud)
library(SnowballC)
```

Next, we need to apply some processing steps to the data:
* convert it to appropriate data type
* remove numbers
* remove commas, exclamation marks, question mars, etc.
* transform all text to lower case
* remove English stopwords, ex. "and", "for" or "or"
* extract radicals, ex. use "baby" instead of "babies"
* remove whitespace

```{r processing, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}

# create a function that will be used on all data types
createCorpusVector <- function(data) {
    data <- Corpus(VectorSource(data))
    data <- tm_map(data, removeNumbers)
    data <- tm_map(data, removePunctuation)
    data <- tm_map(data, tolower)
    data <- tm_map(data, removeWords, stopwords("english"))
    data <- tm_map(data, stemDocument)
    data <- tm_map(data, stripWhitespace)
    data <- tm_map(data, PlainTextDocument)
    data
}

# do the processing
blogs <- createCorpusVector(blogs)
news <- createCorpusVector(news)
twitter <- createCorpusVector(twitter)
```

Analysis
--------

### Word Cloud

First, let's take a look at the most popular words in the dataset.
This will be presented in the form of "word cloud", which is a great way
show word frequencies in easly understandable form.

```{r wordcloud, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}

# function used to display a wordcloud
displayWordcloud <- function(title, data) {
    term.matrix <- DocumentTermMatrix(data, control=list(wordLengths=c(0, Inf)))
    wordcloud(
        words = colnames(term.matrix),
        freq = col_sums(term.matrix), 
        scale = c(3, 1),
        max.words = 50,
        random.order = FALSE,
        rot.per = 0.35,
        use.r.layout = FALSE,
        colors = brewer.pal(11, "Spectral")
    )
    title(title)
}

# show the wordclouds
par(mfrow = c(1, 3))
displayWordcloud("Blogs", blogs)
displayWordcloud("News", news)
displayWordcloud("Tweets", twitter)
```

As we can see from the word clouds, the blogs and tweets are similar 
BLAH BLAH BLAH BLAH


### Bigrams

Finally, we will take a look at the most frequent bigrams (n-grams with n=2)
and trigrams in the dataset.
This will show what kind of phrases are most popular in each data type (blog, news or tweets).

```{r bigrams, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}

# function used to display a barplot of ngrams
displayNGrams <- function(title, data, n) {
    tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = n, max = n))
    ngrams <- TermDocumentMatrix(data, control=list(tokenize=tokenizer))
    ngrams <- sort(col_sums(ngrams), decreasing = TRUE)[1:10]
    barplot <- barplot(
        ngrams,
        #axes = FALSE,
        #axisnames = FALSE,
        ylab = "Count",
        ylim = c(0, max(ngram) + 9),
        main = title
    )
    #text(bar, par("usr")[3], labels = names(bigramTermsCount),
    #     srt = 45, adj = c(1.1,1.1), xpd = TRUE, cex = 0.9)
    #axis(2)    
}

# bigrams
par(mfrow = c(1, 3))
#displayNGrams("Blogs", blogs, 2)
#displayNGrams("News", news, 2)
#displayNGrams("Tweets", twitter, 2)
```

```{r trigrams, cache=TRUE, errors=FALSE, warning=FALSE, message=FALSE, results='asis'}
# trigrams
par(mfrow = c(1, 3))
#displayNGrams("Blogs", blogs, 3)
#displayNGrams("News", news, 3)
#displayNGrams("Tweets", twitter, 3)```